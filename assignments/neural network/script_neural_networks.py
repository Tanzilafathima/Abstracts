# -*- coding: utf-8 -*-
"""assignment neural networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZX9F-Fnd-rI7DaBAOO0jKSxUtAlxNfNN
"""

from google.colab import files
uploaded=files.upload()

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv('forestfires.csv')
df

df.info()

"""Applying PCA due to having more no. of columns

"""

#Scaling the data (leaving out the target variable, and the taking only the numerical data for input)
df1= df.iloc[:,2:30]

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

sc.fit(df1)
df_norm = sc.transform(df1)
df_norm

from sklearn.decomposition import PCA

pca = PCA(n_components = 28)
pca_values = pca.fit_transform(df_norm)
pca_values

# The amount of variance that each PCA explains is 
var = pca.explained_variance_ratio_
var

# Cumulative variance 
var1 = np.cumsum(np.round(var,decimals = 4)*100)
var1

# Variance plot for PCA components obtained
plt.figure(figsize=(12,4))
plt.plot(var1,color="green");

finalDf = pd.concat([pd.DataFrame(pca_values[:,0:24],columns=['pc1','pc2','pc3','pc4','pc5','pc6','pc7',
                                                             'pc8','pc9','pc10','pc11','pc12','pc13','pc14',
                                                             'pc15','pc16','pc17','pc18','pc19','pc20','pc21',
                                                             'pc22','pc23','pc24']),
                     df[['size_category']]], axis = 1)
finalDf.size_category.replace(('large','small'),(1,0),inplace=True)
finalDf

# split into input (X) and output (Y) variables
array = finalDf.values
X = array[:,0:24]
Y = array[:,24]

X.reshape(-1,1)
Y.reshape(-1,1)

# create model
model = Sequential()
model.add(Dense(12, input_dim=24,  activation='relu'))
model.add(Dense(8,  activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fit the model
history = model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)

# evaluate the model
scores = model.evaluate(X, Y)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

history.history.keys()

#summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.xlabel('accuracy')
plt.ylabel('epoch')
plt.legend(['train', 'test'])
plt.show()

#summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.xlabel('loss')
plt.ylabel('epoch')
plt.legend(['train', 'test'])
plt.show()

from google.colab import files
uploaded=files.upload()

df = pd.read_csv('gas_turbines.csv')
df

df.info()

# split into input (X) and output (Y) variables
df1 = df.drop('TEY',axis=1)
df1

finalDf1 = pd.concat([pd.DataFrame(df1),
                     df[['TEY']]], axis = 1)
finalDf1

# split into input (X) and output (Y) variables
array = finalDf.values
X = array[:,0:10]
Y = array[:,10]

X.reshape(-1,1)
Y.reshape(-1,1)

# create model
model = Sequential()
model.add(Dense(12, input_dim=10,  activation='relu'))
model.add(Dense(8,  activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile model
model.compile(loss='mean_squared_error', optimizer='adam')

# Fit the model
history = model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)

# evaluate the model
scores = model.evaluate(X, Y)

history.history.keys()

#summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.xlabel('loss')
plt.ylabel('epoch')
plt.legend(['train', 'test'])
plt.show()

